{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\lokes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 3.4/5.6 MB 18.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 18.0 MB/s eta 0:00:00\n",
      "Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 24.1 MB/s eta 0:00:00\n",
      "Using cached cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, PyPDF2, pycparser, Pillow, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-10.4.0 PyPDF2-3.0.1 cffi-1.17.1 cryptography-43.0.1 pdfminer.six-20231228 pdfplumber-0.11.4 pycparser-2.22 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text extraction is completed. Check file-1.txt and file-2.txt.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 # imported pypdf2 for extracting data from pdf\n",
    "\n",
    "path = \"C:/Users/lokes/Downloads/Harry_Potter_(www.ztcprep.com).pdf\"\n",
    "\n",
    "def extract(path, start, end, output_file): # defined function to extract text from pages\n",
    "    with open(path, 'rb') as file: # opening the pdf file\n",
    "    \n",
    "        pdf_reader = PyPDF2.PdfReader(file) # initlized pdf reader for reading pdf\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as text_f: # opening the output file\n",
    "        \n",
    "            for i in range(start - 1, end): # looping through pages to extract text\n",
    "                if i < num_pages:\n",
    "                    page = pdf_reader.pages[i]\n",
    "                    text = page.extract_text()\n",
    "                    text_f.write(text)\n",
    "                else:\n",
    "                    print(f\"Page {i + 1} is not in the PDF.\")\n",
    "\n",
    "\n",
    "birth_date = 1 # extracting text from page 1 as my dob is -11/01/2001\n",
    "extract(path, birth_date, birth_date + 9, \"file-1.txt\")\n",
    "\n",
    "birth_year = 101 # extracting pages from 101 according to my birth year \n",
    "extract(path, birth_year, birth_year + 9, \"file-2.txt\")\n",
    "\n",
    "print(\"text extraction is completed. Check file-1.txt and file-2.txt.\") # printing the output if it is sucessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 62\n",
      "he: 49\n",
      "a: 39\n",
      "and: 29\n",
      "of: 29\n",
      "was: 25\n",
      "dursley: 23\n",
      "his: 23\n",
      "to: 21\n",
      "that: 20\n",
      "in: 19\n",
      "t: 19\n",
      "mr: 17\n",
      "on: 15\n",
      "it: 15\n",
      "they: 14\n",
      "had: 13\n",
      "as: 13\n",
      "people: 11\n",
      "at: 11\n",
      "www: 10\n",
      "ztcprep: 10\n",
      "com: 10\n",
      "were: 10\n",
      "but: 10\n",
      "mrs: 9\n",
      "didn: 8\n",
      "there: 7\n",
      "potter: 7\n",
      "harry: 7\n",
      "for: 7\n",
      "them: 7\n",
      "back: 7\n",
      "be: 6\n",
      "drills: 6\n",
      "have: 6\n",
      "d: 5\n",
      "called: 5\n",
      "her: 5\n",
      "dudley: 5\n",
      "if: 5\n",
      "out: 5\n",
      "potters: 5\n",
      "s: 5\n",
      "sister: 5\n",
      "what: 5\n",
      "him: 5\n",
      "owl: 5\n",
      "cat: 5\n",
      "boy: 4\n",
      "drive: 4\n",
      "very: 4\n",
      "with: 4\n",
      "lar: 4\n",
      "ge: 4\n",
      "she: 4\n",
      "dursleys: 4\n",
      "son: 4\n",
      "no: 4\n",
      "would: 4\n",
      "about: 4\n",
      "p: 4\n",
      "g: 4\n",
      "e: 4\n",
      "philosophers: 4\n",
      "stone: 4\n",
      "j: 4\n",
      "k: 4\n",
      "rowling: 4\n",
      "good: 4\n",
      "even: 4\n",
      "seen: 4\n",
      "this: 4\n",
      "when: 4\n",
      "up: 4\n",
      "all: 4\n",
      "past: 4\n",
      "something: 4\n",
      "couldn: 4\n",
      "mind: 4\n",
      "who: 3\n",
      "number: 3\n",
      "privet: 3\n",
      "say: 3\n",
      "you: 3\n",
      "or: 3\n",
      "because: 3\n",
      "made: 3\n",
      "man: 3\n",
      "so: 3\n",
      "their: 3\n",
      "think: 3\n",
      "found: 3\n",
      "nothing: 3\n",
      "street: 3\n",
      "never: 3\n",
      "into: 3\n",
      "corner: 3\n",
      "sign: 3\n",
      "wasn: 3\n",
      "been: 3\n",
      "road: 3\n",
      "thought: 3\n",
      "morning: 3\n",
      "lot: 3\n",
      "cloaks: 3\n",
      "see: 3\n",
      "an: 3\n",
      "four: 2\n",
      "perfectly: 2\n",
      "normal: 2\n",
      "much: 2\n",
      "strange: 2\n",
      "mysterious: 2\n",
      "just: 2\n",
      "such: 2\n",
      "grunnings: 2\n",
      "which: 2\n",
      "any: 2\n",
      "neck: 2\n",
      "did: 2\n",
      "mustache: 2\n",
      "usual: 2\n",
      "over: 2\n",
      "neighbors: 2\n",
      "small: 2\n",
      "wanted: 2\n",
      "fear: 2\n",
      "could: 2\n",
      "bear: 2\n",
      "hadn: 2\n",
      "several: 2\n",
      "arrived: 2\n",
      "too: 2\n",
      "away: 2\n",
      "like: 2\n",
      "outside: 2\n",
      "picked: 2\n",
      "most: 2\n",
      "noticed: 2\n",
      "window: 2\n",
      "now: 2\n",
      "little: 2\n",
      "left: 2\n",
      "got: 2\n",
      "reading: 2\n",
      "map: 2\n",
      "then: 2\n",
      "around: 2\n",
      "standing: 2\n",
      "thinking: 2\n",
      "stared: 2\n",
      "drove: 2\n",
      "himself: 2\n",
      "put: 2\n",
      "town: 2\n",
      "by: 2\n",
      "sat: 2\n",
      "traf: 2\n",
      "fic: 2\n",
      "dressed: 2\n",
      "young: 2\n",
      "some: 2\n",
      "stupid: 2\n",
      "these: 2\n",
      "whispering: 2\n",
      "excitedly: 2\n",
      "why: 2\n",
      "collecting: 2\n",
      "yes: 2\n",
      "few: 2\n",
      "always: 2\n",
      "fice: 2\n",
      "might: 2\n",
      "harder: 2\n",
      "concentrate: 2\n",
      "down: 2\n",
      "five: 2\n",
      "telephone: 2\n",
      "until: 2\n",
      "across: 2\n",
      "passed: 2\n",
      "sure: 2\n",
      "lived: 1\n",
      "proud: 1\n",
      "thank: 1\n",
      "last: 1\n",
      "expect: 1\n",
      "involved: 1\n",
      "anything: 1\n",
      "hold: 1\n",
      "nonsense: 1\n",
      "director: 1\n",
      "firm: 1\n",
      "big: 1\n",
      "beefy: 1\n",
      "hardly: 1\n",
      "although: 1\n",
      "thin: 1\n",
      "blonde: 1\n",
      "nearly: 1\n",
      "twice: 1\n",
      "amount: 1\n",
      "came: 1\n",
      "useful: 1\n",
      "spent: 1\n",
      "time: 1\n",
      "craning: 1\n",
      "garden: 1\n",
      "fences: 1\n",
      "spying: 1\n",
      "opinion: 1\n",
      "finer: 1\n",
      "anywhere: 1\n",
      "everything: 1\n",
      "also: 1\n",
      "secret: 1\n",
      "greatest: 1\n",
      "somebody: 1\n",
      "discover: 1\n",
      "anyone: 1\n",
      "2: 1\n",
      "met: 1\n",
      "years: 1\n",
      "fact: 1\n",
      "pretended: 1\n",
      "husband: 1\n",
      "undursleyish: 1\n",
      "possible: 1\n",
      "shuddered: 1\n",
      "knew: 1\n",
      "another: 1\n",
      "reason: 1\n",
      "keeping: 1\n",
      "want: 1\n",
      "mixing: 1\n",
      "child: 1\n",
      "woke: 1\n",
      "dull: 1\n",
      "gray: 1\n",
      "uesday: 1\n",
      "our: 1\n",
      "story: 1\n",
      "starts: 1\n",
      "cloudy: 1\n",
      "sky: 1\n",
      "suggest: 1\n",
      "things: 1\n",
      "soon: 1\n",
      "happening: 1\n",
      "country: 1\n",
      "hummed: 1\n",
      "boring: 1\n",
      "tie: 1\n",
      "work: 1\n",
      "gossiped: 1\n",
      "happily: 1\n",
      "wrestled: 1\n",
      "screaming: 1\n",
      "high: 1\n",
      "chair: 1\n",
      "none: 1\n",
      "tawny: 1\n",
      "flutter: 1\n",
      "half: 1\n",
      "eight: 1\n",
      "briefcase: 1\n",
      "pecked: 1\n",
      "cheek: 1\n",
      "tried: 1\n",
      "kiss: 1\n",
      "bye: 1\n",
      "missed: 1\n",
      "having: 1\n",
      "tantrum: 1\n",
      "throwing: 1\n",
      "cereal: 1\n",
      "walls: 1\n",
      "tyke: 1\n",
      "chortled: 1\n",
      "house: 1\n",
      "car: 1\n",
      "backed: 1\n",
      "first: 1\n",
      "peculiar: 1\n",
      "second: 1\n",
      "realize: 1\n",
      "jerked: 1\n",
      "head: 1\n",
      "look: 1\n",
      "again: 1\n",
      "tabby: 1\n",
      "sight: 1\n",
      "must: 1\n",
      "trick: 1\n",
      "light: 1\n",
      "blinked: 1\n",
      "3: 1\n",
      "watched: 1\n",
      "mirror: 1\n",
      "said: 1\n",
      "looking: 1\n",
      "cats: 1\n",
      "read: 1\n",
      "maps: 1\n",
      "signs: 1\n",
      "gave: 1\n",
      "shake: 1\n",
      "toward: 1\n",
      "except: 1\n",
      "order: 1\n",
      "hoping: 1\n",
      "get: 1\n",
      "day: 1\n",
      "edge: 1\n",
      "driven: 1\n",
      "else: 1\n",
      "jam: 1\n",
      "help: 1\n",
      "noticing: 1\n",
      "seemed: 1\n",
      "strangely: 1\n",
      "funny: 1\n",
      "clothes: 1\n",
      "getups: 1\n",
      "saw: 1\n",
      "supposed: 1\n",
      "new: 1\n",
      "fashion: 1\n",
      "drummed: 1\n",
      "fingers: 1\n",
      "steering: 1\n",
      "wheel: 1\n",
      "eyes: 1\n",
      "fell: 1\n",
      "huddle: 1\n",
      "weirdos: 1\n",
      "quite: 1\n",
      "close: 1\n",
      "together: 1\n",
      "enraged: 1\n",
      "couple: 1\n",
      "weren: 1\n",
      "older: 1\n",
      "than: 1\n",
      "wearing: 1\n",
      "emerald: 1\n",
      "green: 1\n",
      "cloak: 1\n",
      "nerve: 1\n",
      "struck: 1\n",
      "probably: 1\n",
      "silly: 1\n",
      "stunt: 1\n",
      "obviously: 1\n",
      "moved: 1\n",
      "minutes: 1\n",
      "later: 1\n",
      "parking: 1\n",
      "ninth: 1\n",
      "floor: 1\n",
      "owls: 1\n",
      "swooping: 1\n",
      "broad: 1\n",
      "daylight: 1\n",
      "though: 1\n",
      "pointed: 1\n",
      "gazed: 1\n",
      "open: 1\n",
      "mouthed: 1\n",
      "after: 1\n",
      "sped: 1\n",
      "overhead: 1\n",
      "nighttime: 1\n",
      "however: 1\n",
      "free: 1\n",
      "yelled: 1\n",
      "dif: 1\n",
      "ferent: 1\n",
      "important: 1\n",
      "4: 1\n",
      "calls: 1\n",
      "shouted: 1\n",
      "bit: 1\n",
      "more: 1\n",
      "mood: 1\n",
      "lunchtime: 1\n",
      "stretch: 1\n",
      "legs: 1\n",
      "walk: 1\n",
      "buy: 1\n",
      "bun: 1\n",
      "from: 1\n",
      "bakery: 1\n",
      "gotten: 1\n",
      "group: 1\n",
      "next: 1\n",
      "baker: 1\n",
      "eyed: 1\n",
      "angrily: 1\n",
      "know: 1\n",
      "uneasy: 1\n",
      "bunch: 1\n",
      "single: 1\n",
      "tin: 1\n",
      "way: 1\n",
      "clutching: 1\n",
      "doughnut: 1\n",
      "bag: 1\n",
      "caught: 1\n",
      "words: 1\n",
      "saying: 1\n",
      "right: 1\n",
      "i: 1\n",
      "heard: 1\n",
      "stopped: 1\n",
      "dead: 1\n",
      "flooded: 1\n",
      "looked: 1\n",
      "whisperers: 1\n",
      "better: 1\n",
      "dashed: 1\n",
      "hurried: 1\n",
      "snapped: 1\n",
      "secretary: 1\n",
      "not: 1\n",
      "disturb: 1\n",
      "seized: 1\n",
      "almost: 1\n",
      "finished: 1\n",
      "dialing: 1\n",
      "home: 1\n",
      "changed: 1\n",
      "receiver: 1\n",
      "stroked: 1\n",
      "being: 1\n",
      "unusual: 1\n",
      "name: 1\n",
      "lots: 1\n",
      "come: 1\n",
      "nephew: 1\n",
      "harvey: 1\n",
      "harold: 1\n",
      "point: 1\n",
      "worrying: 1\n",
      "upset: 1\n",
      "mention: 1\n",
      "blame: 1\n",
      "same: 1\n",
      "those: 1\n",
      "5: 1\n",
      "afternoon: 1\n",
      "building: 1\n",
      "o: 1\n",
      "clock: 1\n",
      "still: 1\n",
      "worried: 1\n",
      "walked: 1\n",
      "straight: 1\n",
      "someone: 1\n",
      "door: 1\n",
      "sorry: 1\n",
      "grunted: 1\n",
      "tiny: 1\n",
      "old: 1\n",
      "stumbled: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Function to read the content of a file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Map function to split text into words and assign an initial count of 1 to each\n",
    "def map_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Split by words and convert to lowercase\n",
    "    word_map = [(word, 1) for word in words]\n",
    "    return word_map\n",
    "\n",
    "# Reduce function to count occurrences of each word\n",
    "def reduce_word_counts(mapped_words):\n",
    "    word_counts = defaultdict(int)\n",
    "    for word, count in mapped_words:\n",
    "        word_counts[word] += count\n",
    "    return word_counts\n",
    "\n",
    "# Read the file1.txt content\n",
    "file1_text = read_file(\"file1.txt\")\n",
    "\n",
    "# Map step: Split the text into words\n",
    "mapped_words = map_words(file1_text)\n",
    "\n",
    "# Reduce step: Count occurrences of each word\n",
    "word_counts = reduce_word_counts(mapped_words)\n",
    "\n",
    "# Print the word counts\n",
    "for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 3.1/6.8 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 18.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English words and their counts:\n",
      "hagrid: 27\n",
      "ter: 19\n",
      "www: 10\n",
      "ztcprep: 10\n",
      "com: 10\n",
      "yeh: 10\n",
      "dudley: 7\n",
      "rowling: 7\n",
      "ll: 7\n",
      "hogwarts: 7\n",
      "ernon: 6\n",
      "didn: 6\n",
      "gringotts: 5\n",
      "dumbledore: 4\n",
      "hadn: 3\n",
      "ve: 3\n",
      "stuf: 3\n",
      "ap: 3\n",
      "wasn: 2\n",
      "albus: 2\n",
      "gettin: 2\n",
      "knuts: 2\n",
      "izards: 2\n",
      "eah: 2\n",
      "64: 1\n",
      "muggle: 1\n",
      "james: 1\n",
      "goin: 1\n",
      "dumbled: 1\n",
      "ying: 1\n",
      "insul: 1\n",
      "65: 1\n",
      "shouldn: 1\n",
      "speakin: 1\n",
      "aren: 1\n",
      "meself: 1\n",
      "66: 1\n",
      "ou: 1\n",
      "67: 1\n",
      "diagon: 1\n",
      "ther: 1\n",
      "68: 1\n",
      "ge: 1\n",
      "payin: 1\n",
      "deliverin: 1\n",
      "69: 1\n",
      "london: 1\n",
      "mm: 1\n",
      "wouldn: 1\n",
      "teh: 1\n",
      "70: 1\n",
      "cept: 1\n",
      "fetchin: 1\n",
      "everythin: 1\n",
      "pposed: 1\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "# Function to identify non-English words using SpellChecker\n",
    "def find_non_english_words(text):\n",
    "    spell = SpellChecker()\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Split by words\n",
    "    non_english_words = [word for word in words if word not in spell]\n",
    "    return non_english_words\n",
    "\n",
    "# Read the file2.txt content\n",
    "file2_text = read_file(\"file2.txt\")\n",
    "\n",
    "# Find non-English words\n",
    "non_english_words = find_non_english_words(file2_text)\n",
    "\n",
    "# Count occurrences of non-English words using MapReduce\n",
    "mapped_non_english_words = map_words(' '.join(non_english_words))\n",
    "non_english_word_counts = reduce_word_counts(mapped_non_english_words)\n",
    "\n",
    "# Print the non-English words and their counts\n",
    "print(\"Non-English words and their counts:\")\n",
    "for word, count in sorted(non_english_word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
